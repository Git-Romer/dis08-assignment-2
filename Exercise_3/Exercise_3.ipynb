{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This Notebook contains all solutions from the Exercise 3\n",
    "\n",
    "## Nr. 1\n",
    "\n",
    "- Pick a list within the Wikipedia like the list of sovereign states. Choose some other list on your own, based on your personal interests. The only requirement is that there are other Wikipedia articles linked within the list.\n",
    "\n",
    "Since we are hopeless drunkards and celebrate a lot (irony) we decided to use a list that gives an overview of the countries that have a ban on alcohol.\n",
    "\n",
    "**URL: [https://en.wikipedia.org/wiki/List_of_countries_with_alcohol_prohibition](https://en.wikipedia.org/wiki/List_of_countries_with_alcohol_prohibition)**\n",
    "\n",
    "## Nr.2\n",
    "\n",
    "- Get all the names and URLs to the corresponding items in the list and export them into a CSV file that has two columns (name and URL)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "url = \"https://en.wikipedia.org/wiki/List_of_countries_with_alcohol_prohibition\"\n",
    "content = requests.get(url).text\n",
    "soup = BeautifulSoup(content)\n",
    "def titles(soup):\n",
    "    present_list = soup.find_all(\"ul\")[1]\n",
    "    titles = []\n",
    "    for x in present_list.find_all(\"a\"):\n",
    "        titles.append(x.get(\"title\"))\n",
    "    titles = list(set(titles))\n",
    "    remove = [None, \"Indonesia\", \"COVID-19 pandemic in South Africa\", \"India\", \"South Yemen\", \"Union Territory\", \"Sharjah (emirate)\"]\n",
    "    for things in remove:\n",
    "        titles.remove(things)\n",
    "    titles_clean = sorted(titles)\n",
    "    return titles_clean\n",
    "def href(titles):\n",
    "    url = \"https://wikipedia.org/wiki/\"\n",
    "    href = []\n",
    "    links = []\n",
    "    for country in titles:\n",
    "        href.append(url)\n",
    "    for i in range(len(href)):\n",
    "        links.append(list(href[i] + titles[x] for x in range(len(titles))))\n",
    "    links = links.pop(0)\n",
    "    links_clean = []\n",
    "    for entries in links:\n",
    "        re = entries.replace(\" \", \"_\")\n",
    "        links_clean.append(re)\n",
    "    return links_clean\n",
    "def writer(title, link):\n",
    "    column_names = [\"Names\", \"URL\"]\n",
    "    with open(\"List_of_countries_with_alcohol_prohibition.csv\", \"w\", newline = \"\") as csvfile:\n",
    "        writer = csv.writer(csvfile, delimiter=\",\")\n",
    "        writer.writerow(column_names)\n",
    "        writer.writerows(zip(title, link))\n",
    "titles(soup)\n",
    "href(titles(soup))\n",
    "writer(titles(soup), href(titles(soup)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nr. 3\n",
    "\n",
    "- For every Wikipedia article in the CSV list choose a few attributes from the infobox on the right that you would like to extract (e.g., population, name of the head of state, whatever...). Extract this information for every entry in your list. Store this information in an appropriate data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import pandas as pd\n",
    "import requests\n",
    "import re\n",
    "import lxml\n",
    "url = \"https://en.wikipedia.org/wiki/List_of_countries_with_alcohol_prohibition\"\n",
    "content = requests.get(url).text\n",
    "soup = BeautifulSoup(content)\n",
    "def titles(soup):\n",
    "    present_list = soup.find_all(\"ul\")[1]\n",
    "    titles = []\n",
    "    for x in present_list.find_all(\"a\"):\n",
    "        titles.append(x.get(\"title\"))\n",
    "    titles = list(set(titles))\n",
    "    remove = [None, \"Indonesia\", \"COVID-19 pandemic in South Africa\", \"India\", \"South Yemen\", \"Union Territory\", \"Sharjah (emirate)\"]\n",
    "    for things in remove:\n",
    "        titles.remove(things)\n",
    "    titles_clean = sorted(titles)\n",
    "    return titles_clean\n",
    "def href(titles):\n",
    "    url = \"https://wikipedia.org/wiki/\"\n",
    "    href = []\n",
    "    links = []\n",
    "    for countrie in titles:\n",
    "        href.append(url)\n",
    "    for i in range(len(href)):\n",
    "        links.append(list(href[i] + titles[x] for x in range(len(titles))))\n",
    "    links = links.pop(0)\n",
    "    links_clean = []\n",
    "    for entries in links:\n",
    "        repl = entries.replace(\" \", \"_\")\n",
    "        links_clean.append(repl)\n",
    "    return links_clean\n",
    "titles(soup)\n",
    "href(titles(soup))\n",
    "def capital(link):\n",
    "    content = requests.get(link).text\n",
    "    soup = BeautifulSoup(content, \"lxml\")\n",
    "    infobox = soup.find(\"table\", attrs={\"class\": \"infobox geography vcard\"}).tbody\n",
    "    rows = infobox.find_all(\"tr\")\n",
    "    capitals = []\n",
    "    for x in rows:\n",
    "        if re.search(\"Capital\", str(x)) != None:\n",
    "            if x.find(\"a\").get_text() == \"Capital\":\n",
    "                capitals.append(x.find_all([\"a\"])[1].get_text())\n",
    "            else:\n",
    "                capitals.append(x.find(\"a\").get_text())\n",
    "    capitals_clean = []\n",
    "    for entries in capitals:\n",
    "        repl = entries.replace(\" \", \"_\")\n",
    "        capitals_clean.append(repl)\n",
    "    for x in capitals_clean:\n",
    "        return x\n",
    "def timezone(link):\n",
    "    content = requests.get(link).text\n",
    "    soup = BeautifulSoup(content, \"lxml\")\n",
    "    infobox = soup.find(\"table\", attrs={\"class\": \"infobox geography vcard\"}).tbody\n",
    "    rows = infobox.find_all(\"tr\")\n",
    "    timezone = []\n",
    "    for x in rows:\n",
    "        if re.search(\"Time zone\", str(x)) != None:\n",
    "            if x.find([\"span\", \"a\"]).get_text() == \"Time zone\":\n",
    "                timezone.append(x.find_all([\"span\", \"a\"])[1].get_text())\n",
    "            else:\n",
    "                timezone.append(x.find([\"span\", \"a\"]).get_text())\n",
    "    timezones_clean = []\n",
    "    for entries in timezone:\n",
    "        repl = entries.replace(\" \", \"_\")\n",
    "        timezones_clean.append(repl)\n",
    "    for x in timezones_clean:\n",
    "        return x\n",
    "def writer(title, link, capitals, timezones):\n",
    "    column_names = [\"Names\", \"URL\", \"Capitals\", \"Timezones\"]\n",
    "    with open(\"List_of_countries_with_alcohol_prohibition.csv\", \"w\", newline = \"\") as csvfile:\n",
    "        writer = csv.writer(csvfile, delimiter=\",\")\n",
    "        writer.writerow(column_names)\n",
    "        writer.writerows(zip(title, link, capitals, timezones))\n",
    "capitals = []\n",
    "timezones = []\n",
    "df = pd.read_csv(\"List_of_countries_with_alcohol_prohibition.csv\", delimiter = \",\")\n",
    "for link in df[\"URL\"]:\n",
    "    capitals.append(capital(link))\n",
    "    timezones.append(timezone(link))\n",
    "writer(titles(soup), href(titles(soup)), capitals, timezones)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nr. 4\n",
    "\n",
    "- Save your scraped information into a JSON file. Try to export clean data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3810"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import csv\n",
    "import json\n",
    "CSV_PATH = './List_of_countries_with_alcohol_prohibition.csv'\n",
    "JSON_PATH = './List_of_countries_with_alcohol_prohibition.json'\n",
    "csv_file = csv.DictReader(open(CSV_PATH, 'r'))\n",
    "json_list = []\n",
    "for row in csv_file:\n",
    "    json_list.append(row)\n",
    "open(JSON_PATH, 'w').write(json.dumps(json_list, indent=4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
