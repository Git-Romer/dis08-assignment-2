{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This Notebook contains all solutions from the Exercise 3\n",
    "\n",
    "## Nr. 1\n",
    "\n",
    "- Pick a list within the Wikipedia like the list of sovereign states. Choose some other list on your own, based on your personal interests. The only requirement is that there are other Wikipedia articles linked within the list.\n",
    "\n",
    "Since we are hopeless drunkards and celebrate a lot (irony) we decided to use a list that gives an overview of the countries that have a ban on alcohol.\n",
    "\n",
    "**URL: [https://en.wikipedia.org/wiki/List_of_countries_with_alcohol_prohibition](https://en.wikipedia.org/wiki/List_of_countries_with_alcohol_prohibition)**\n",
    "\n",
    "## Nr.2\n",
    "\n",
    "- Get all the names and URLs to the corresponding items in the list and export them into a CSV file that has two columns (name and URL).\n",
    "\n",
    "To access the Wikipedia data we use the module [`Beautiful Soup`](https://www.crummy.com/software/BeautifulSoup/bs4/doc/) together with the module [`requests`](https://realpython.com/python-requests/). This allows us to work in real time with a copy of the source code of the page we want to access. To structure the program more clearly we have declared functions that describe the processes. Little by little we get the necessary information. First the names of the countries, then the URL to the Wikipedia articles and finally the CSV file is written.  \n",
    "\n",
    "**File: [List_of_countries_with_alcohol_prohibition.csv](./List_of_countries_with_alcohol_prohibition.csv)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing modules\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "# URL of Wiki list\n",
    "url = \"https://en.wikipedia.org/wiki/List_of_countries_with_alcohol_prohibition\"\n",
    "# Reading in the source code\n",
    "content = requests.get(url).text\n",
    "soup = BeautifulSoup(content)\n",
    "# Finding all the Country names\n",
    "def titles(soup):\n",
    "    # Searching for the second unsorted list\n",
    "    present_list = soup.find_all(\"ul\")[1]\n",
    "    titles = []\n",
    "    # Fetching the title attribute containing the country name\n",
    "    for x in present_list.find_all(\"a\"):\n",
    "        titles.append(x.get(\"title\"))\n",
    "    # Creating a unique list in case there are duples\n",
    "    titles = list(set(titles))\n",
    "    # Removing everything that is not a country or that is more defined\n",
    "    remove = [None, \"Indonesia\", \"COVID-19 pandemic in South Africa\", \"India\", \"South Yemen\", \"Union Territory\", \"Sharjah (emirate)\"]\n",
    "    for things in remove:\n",
    "        titles.remove(things)\n",
    "    # Sorting the list\n",
    "    titles_clean = sorted(titles)\n",
    "    return titles_clean\n",
    "# Finding all URL's\n",
    "def href(titles):\n",
    "    # Base for the URL\n",
    "    url = \"https://wikipedia.org/wiki/\"\n",
    "    href = []\n",
    "    links = []\n",
    "    # Iterating through all titles and adding the base URL\n",
    "    for country in titles:\n",
    "        href.append(url)\n",
    "    # Adding the specific URL name to the base\n",
    "    for i in range(len(href)):\n",
    "        links.append(list(href[i] + titles[x] for x in range(len(titles))))\n",
    "    # Removing duples lists\n",
    "    links = links.pop(0)\n",
    "    links_clean = []\n",
    "    # Replacing every whitespace for a underscore\n",
    "    for entries in links:\n",
    "        re = entries.replace(\" \", \"_\")\n",
    "        links_clean.append(re)\n",
    "    return links_clean\n",
    "# Writing the CSV File\n",
    "def writer(title, link):\n",
    "    column_names = [\"Names\", \"URL\"]\n",
    "    with open(\"List_of_countries_with_alcohol_prohibition.csv\", \"w\", newline = \"\") as csvfile:\n",
    "        writer = csv.writer(csvfile, delimiter=\",\")\n",
    "        writer.writerow(column_names)\n",
    "        # Zipping both lists together as they have to be in two seperate columns\n",
    "        writer.writerows(zip(title, link))\n",
    "titles(soup)\n",
    "href(titles(soup))\n",
    "writer(titles(soup), href(titles(soup)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nr. 3\n",
    "\n",
    "- For every Wikipedia article in the CSV list choose a few attributes from the infobox on the right that you would like to extract (e.g., population, name of the head of state, whatever...). Extract this information for every entry in your list. Store this information in an appropriate data structure.\n",
    "\n",
    "Now we come to the somewhat more difficult part of the task. In order to find out the individual values of the countries we have to execute the program for each row. For this task even the program needs some seconds. As a basis the program written before in no. 2 is used, because otherwise the CSV file can not be written correctly, since the attributes `titles` and `href` are missing. To find out the capital cities (`capital`) and the time values (`timezone`) we proceeded similarly as in the task solved before. Finally the CSV file was overwritten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0xe9 in position 3: unexpected end of data",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_tokens\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_with_dtype\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._string_convert\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers._string_box_utf8\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0xe9 in position 3: unexpected end of data",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-2b60e8c82609>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[0mtimezones\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m \u001b[1;31m# Creating a dataframe for the CSV table created in Ex. 3.2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 117\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"List_of_countries_with_alcohol_prohibition.csv\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdelimiter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\",\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    118\u001b[0m \u001b[1;31m# Iterating through every Wiki link in the CSV file from Ex. 3.2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mlink\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"URL\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    674\u001b[0m         )\n\u001b[0;32m    675\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 676\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    678\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    452\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    453\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 454\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    455\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    456\u001b[0m         \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1131\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m         \u001b[0mnrows\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_validate_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"nrows\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1133\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1134\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1135\u001b[0m         \u001b[1;31m# May alter columns / col_dict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   2035\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2036\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2037\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2038\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2039\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_first_chunk\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_low_memory\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_column_data\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_tokens\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_with_dtype\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._string_convert\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers._string_box_utf8\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0xe9 in position 3: unexpected end of data"
     ]
    }
   ],
   "source": [
    "# Importing modules\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import pandas as pd\n",
    "import requests\n",
    "import re\n",
    "import lxml\n",
    "# URL of Wiki list\n",
    "url = \"https://en.wikipedia.org/wiki/List_of_countries_with_alcohol_prohibition\"\n",
    "# Reading in the source code\n",
    "content = requests.get(url).text\n",
    "soup = BeautifulSoup(content)\n",
    "# Finding all the Country names\n",
    "def titles(soup):\n",
    "    # Searching for the second unsorted list\n",
    "    present_list = soup.find_all(\"ul\")[1]\n",
    "    titles = []\n",
    "    # Fetching the title attribute containing the country name\n",
    "    for x in present_list.find_all(\"a\"):\n",
    "        titles.append(x.get(\"title\"))\n",
    "    # Creating a unique list in case there are duples\n",
    "    titles = list(set(titles))\n",
    "    # Removing everything that is not a country or that is more defined\n",
    "    remove = [None, \"Indonesia\", \"COVID-19 pandemic in South Africa\", \"India\", \"South Yemen\", \"Union Territory\", \"Sharjah (emirate)\"]\n",
    "    # Creating a sorted clean list\n",
    "    for things in remove:\n",
    "        titles.remove(things)\n",
    "    titles_clean = sorted(titles)\n",
    "    return titles_clean\n",
    "# Finding all URL's\n",
    "def href(titles):\n",
    "    # Base for the URL\n",
    "    url = \"https://wikipedia.org/wiki/\"\n",
    "    href = []\n",
    "    links = []\n",
    "    # Iterating through all titles and adding the base URL\n",
    "    for countrie in titles:\n",
    "        href.append(url)\n",
    "    # Adding the specific URL name to the base\n",
    "    for i in range(len(href)):\n",
    "        links.append(list(href[i] + titles[x] for x in range(len(titles))))\n",
    "    # Removing duples lists\n",
    "    links = links.pop(0)\n",
    "    links_clean = []\n",
    "    # Replacing every whitespace for a underscore\n",
    "    for entries in links:\n",
    "        repl = entries.replace(\" \", \"_\")\n",
    "        links_clean.append(repl)\n",
    "    return links_clean\n",
    "# Calling the functions\n",
    "titles(soup)\n",
    "href(titles(soup))\n",
    "# Finding all capitals\n",
    "def capital(link):\n",
    "    # Reading in the source code\n",
    "    content = requests.get(link).text\n",
    "    soup = BeautifulSoup(content, \"lxml\")\n",
    "    # Shrinking down source code\n",
    "    infobox = soup.find(\"table\", attrs={\"class\": \"infobox geography vcard\"}).tbody\n",
    "    # Searching for all the table rows\n",
    "    rows = infobox.find_all(\"tr\")\n",
    "    capitals = []\n",
    "    # Iterating over all rows\n",
    "    for x in rows:\n",
    "        # Ignoring all cases in which there are no matches\n",
    "        if re.search(\"Capital\", str(x)) != None:\n",
    "            # Ignoring all cases in which the capital is \"capital\"\n",
    "            if x.find(\"a\").get_text() == \"Capital\":\n",
    "                capitals.append(x.find_all([\"a\"])[1].get_text())\n",
    "            else:\n",
    "                capitals.append(x.find(\"a\").get_text())\n",
    "    capitals_clean = []\n",
    "    # Replacing every whitespace for a underscore\n",
    "    for entries in capitals:\n",
    "        repl = entries.replace(\" \", \"_\")\n",
    "        capitals_clean.append(repl)\n",
    "    for x in capitals_clean:\n",
    "        return x\n",
    "# Finding all time zones\n",
    "def timezone(link):\n",
    "    # Reading in the source code\n",
    "    content = requests.get(link).text\n",
    "    soup = BeautifulSoup(content, \"lxml\")\n",
    "    # Shrinking down source code\n",
    "    infobox = soup.find(\"table\", attrs={\"class\": \"infobox geography vcard\"}).tbody\n",
    "    # Searching for all the table rows\n",
    "    rows = infobox.find_all(\"tr\")\n",
    "    timezone = []\n",
    "    # Iterating over all rows\n",
    "    for x in rows:\n",
    "        # Ignoring all cases in which there are no matches\n",
    "        if re.search(\"Time zone\", str(x)) != None:\n",
    "            # Ignoring all cases in which the time zone is \"Time zone\"\n",
    "            if x.find([\"span\", \"a\"]).get_text() == \"Time zone\":\n",
    "                timezone.append(x.find_all([\"span\", \"a\"])[1].get_text())\n",
    "            else:\n",
    "                timezone.append(x.find([\"span\", \"a\"]).get_text())\n",
    "    timezones_clean = []\n",
    "    # Replacing every whitespace for a underscore\n",
    "    for entries in timezone:\n",
    "        repl = entries.replace(\" \", \"_\")\n",
    "        timezones_clean.append(repl)\n",
    "    for x in timezones_clean:\n",
    "        return x\n",
    "# Writing the CSV File\n",
    "def writer(title, link, capitals, timezones):\n",
    "    column_names = [\"Names\", \"URL\", \"Capitals\", \"Timezones\"]\n",
    "    with open(\"List_of_countries_with_alcohol_prohibition.csv\", \"w\", newline = \"\") as csvfile:\n",
    "        writer = csv.writer(csvfile, delimiter=\",\")\n",
    "        writer.writerow(column_names)\n",
    "        # Zipping all the lists together as they have to be in seperate columns\n",
    "        writer.writerows(zip(title, link, capitals, timezones))\n",
    "# Calling the functions\n",
    "capitals = []\n",
    "timezones = []\n",
    "# Creating a dataframe for the CSV table created in Ex. 3.2\n",
    "df = pd.read_csv(\"List_of_countries_with_alcohol_prohibition.csv\", delimiter = \",\")\n",
    "# Iterating through every Wiki link in the CSV file from Ex. 3.2\n",
    "for link in df[\"URL\"]:\n",
    "    # Calling the functions with every link\n",
    "    capitals.append(capital(link))\n",
    "    timezones.append(timezone(link))\n",
    "# Calling the writer function with all variables\n",
    "writer(titles(soup), href(titles(soup)), capitals, timezones)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nr. 4\n",
    "\n",
    "- Save your scraped information into a JSON file. Try to export clean data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3810"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import csv\n",
    "import json\n",
    "CSV_PATH = './List_of_countries_with_alcohol_prohibition.csv'\n",
    "JSON_PATH = './List_of_countries_with_alcohol_prohibition.json'\n",
    "csv_file = csv.DictReader(open(CSV_PATH, 'r'))\n",
    "json_list = []\n",
    "for row in csv_file:\n",
    "    json_list.append(row)\n",
    "open(JSON_PATH, 'w').write(json.dumps(json_list, indent=4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
